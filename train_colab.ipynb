{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Electronics Detection Model Training - Google Colab\n",
    "\n",
    "Train YOLOv8 model for electronics detection using **FREE GPU** on Google Colab.\n",
    "\n",
    "**Estimated Training Time:** 2-3 hours (vs 24+ hours on CPU locally)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "1. ‚úÖ Download your dataset (e.g., from [Roboflow Universe](https://universe.roboflow.com/sanctum/electronics-j0cxl))\n",
    "2. ‚úÖ Upload dataset to Google Drive (recommended) or upload directly to Colab\n",
    "3. ‚úÖ Enable GPU: **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU Availability\n",
    "\n",
    "Verify that GPU is enabled (should show Tesla T4 or similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Install Ultralytics (YOLOv8) and required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "\n",
    "# Import libraries\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Mount Google Drive (Optional but Recommended)\n",
    "\n",
    "Mount your Google Drive to access the dataset and save results.\n",
    "\n",
    "**Skip this cell if you'll upload the dataset directly to Colab.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted at /content/drive/MyDrive/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Upload Dataset\n",
    "\n",
    "### Option A: From Google Drive (Recommended)\n",
    "\n",
    "If you uploaded your dataset to Google Drive, update the path below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update this path to match your dataset location in Google Drive\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Electronics.v1i.yolov8\"\n",
    "\n",
    "# Verify dataset exists\n",
    "if os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚úÖ Dataset found at: {DATASET_PATH}\")\n",
    "    print(f\"üìÅ Contents: {os.listdir(DATASET_PATH)}\")\n",
    "else:\n",
    "    print(f\"‚ùå Dataset not found at: {DATASET_PATH}\")\n",
    "    print(\"Please update DATASET_PATH or use Option B to upload directly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B: Upload Dataset Directly to Colab\n",
    "\n",
    "**Alternative:** Upload your dataset.zip file using the file upload button below.\n",
    "\n",
    "‚ö†Ô∏è **Note:** Files uploaded directly to Colab will be deleted when the session ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if uploading directly\n",
    "\n",
    "# from google.colab import files\n",
    "# import zipfile\n",
    "\n",
    "# print(\"üì§ Upload your dataset.zip file...\")\n",
    "# uploaded = files.upload()\n",
    "\n",
    "# # Extract the uploaded zip\n",
    "# for filename in uploaded.keys():\n",
    "#     if filename.endswith('.zip'):\n",
    "#         print(f\"üì¶ Extracting {filename}...\")\n",
    "#         with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "#             zip_ref.extractall('/content/')\n",
    "#         dataset_name = filename.replace('.zip', '')\n",
    "#         DATASET_PATH = f\"/content/{dataset_name}\"\n",
    "#         print(f\"‚úÖ Dataset extracted to: {DATASET_PATH}\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Configure Training Parameters & Auto-Resume\n",
    "\n",
    "Adjust these settings based on your needs.\n",
    "\n",
    "‚ö†Ô∏è **Free Tier Protection:** The notebook will automatically:\n",
    "- Save checkpoints to Google Drive every N epochs\n",
    "- Resume from the last checkpoint if training is interrupted\n",
    "- Protect against session timeouts and disconnections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "CONFIG = {\n",
    "    'model': 'yolov8n.pt',        # Base model (n=nano, s=small, m=medium)\n",
    "    'custom_model': None,          # Path to your custom model for incremental training (e.g., '/content/drive/MyDrive/best.pt')\n",
    "    'epochs': 100,                 # Number of training epochs\n",
    "    'batch': 16,                   # Batch size (reduce if out of memory)\n",
    "    'imgsz': 640,                  # Image size\n",
    "    'device': 0,                   # GPU device (0 for first GPU)\n",
    "    'patience': 20,                # Early stopping patience\n",
    "    'save_period': 5,              # Save checkpoint every N epochs (reduced for free tier)\n",
    "    'project': 'electronics_training',  # Output folder\n",
    "    'name': 'colab_run',           # Experiment name\n",
    "    \n",
    "    # üÜï AUTO-RESUME SETTINGS (for Colab free tier)\n",
    "    'auto_resume': True,           # Automatically resume from last checkpoint\n",
    "    'drive_backup': True,          # Backup checkpoints to Google Drive\n",
    "    'drive_backup_path': '/content/drive/MyDrive/colab_training_backups',  # Google Drive backup location\n",
    "}\n",
    "\n",
    "# üìù INCREMENTAL TRAINING INSTRUCTIONS:\n",
    "# To train from a previously trained model instead of starting from scratch:\n",
    "# 1. Upload your trained model (e.g., best.pt or last.pt) to Google Drive\n",
    "# 2. Set 'custom_model' to the path of your model, e.g.:\n",
    "#    CONFIG['custom_model'] = '/content/drive/MyDrive/electronics_training/colab_run/weights/best.pt'\n",
    "# 3. Run the training - it will continue from your custom model!\n",
    "\n",
    "# üîÑ AUTO-RESUME INSTRUCTIONS:\n",
    "# If your Colab session times out or disconnects:\n",
    "# 1. Simply re-run all cells - the notebook will automatically detect the last checkpoint\n",
    "# 2. Training will resume from where it left off (no progress lost!)\n",
    "# 3. Checkpoints are saved to Google Drive every 5 epochs\n",
    "\n",
    "print(\"‚öôÔ∏è  Training Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "    \n",
    "if CONFIG['custom_model']:\n",
    "    print(\"\\nüîÑ INCREMENTAL TRAINING MODE\")\n",
    "    print(f\"   Base model: {CONFIG['custom_model']}\")\n",
    "    print(\"   Training will continue from this checkpoint!\")\n",
    "else:\n",
    "    print(f\"\\nüÜï FRESH TRAINING MODE\")\n",
    "    print(f\"   Starting from: {CONFIG['model']}\")\n",
    "\n",
    "if CONFIG['auto_resume']:\n",
    "    print(f\"\\n‚úÖ AUTO-RESUME ENABLED\")\n",
    "    print(f\"   Checkpoints will be saved to: {CONFIG['drive_backup_path']}\")\n",
    "    print(f\"   If session disconnects, just re-run and training will resume!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.5: (Optional) Keep Session Alive\n",
    "\n",
    "‚ö†Ô∏è **For Free Tier Users:** Colab may disconnect after ~90 minutes of inactivity.\n",
    "\n",
    "Run the cell below to add a keep-alive script (optional but recommended for long training sessions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep-Alive Script (prevents Colab timeout)\n",
    "# This simulates user activity to keep the session alive\n",
    "\n",
    "from IPython.display import display, Javascript\n",
    "\n",
    "display(Javascript('''\n",
    "function KeepClicking(){\n",
    "    console.log(\"Keep-Alive: Simulating activity...\");\n",
    "    document.querySelector(\"colab-connect-button\").click();\n",
    "}\n",
    "setInterval(KeepClicking, 60000);  // Click every 60 seconds\n",
    "'''))\n",
    "\n",
    "print(\"‚úÖ Keep-Alive script activated!\")\n",
    "print(\"   Session will stay active even during long training runs\")\n",
    "print(\"   (Simulates activity every 60 seconds)\")\n",
    "print(\"\\n‚ö†Ô∏è  Note: Even with keep-alive, Colab may disconnect after 12 hours\")\n",
    "print(\"   But don't worry - auto-resume will save your progress!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ How Auto-Resume Works\n",
    "\n",
    "### If Your Session Disconnects:\n",
    "\n",
    "1. **Don't Panic!** Your progress is automatically saved to Google Drive every 5 epochs\n",
    "2. **Re-run the notebook:** Just click \"Runtime ‚Üí Run all\" \n",
    "3. **Automatic Resume:** The notebook will detect your last checkpoint and continue from there\n",
    "\n",
    "### What Gets Saved:\n",
    "- ‚úÖ Model weights (`last.pt` and `best.pt`)\n",
    "- ‚úÖ Training progress (epoch number, optimizer state)\n",
    "- ‚úÖ All training results and metrics\n",
    "\n",
    "### Backup Location:\n",
    "Your checkpoints are saved to: `/content/drive/MyDrive/colab_training_backups/`\n",
    "\n",
    "### Manual Resume (Advanced):\n",
    "If you want to manually resume from a specific checkpoint:\n",
    "```python\n",
    "CONFIG['custom_model'] = '/content/drive/MyDrive/colab_training_backups/colab_run_last.pt'\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup periodic backup during training (runs in background)\n",
    "import threading\n",
    "import time\n",
    "\n",
    "def periodic_backup():\n",
    "    \"\"\"Background thread that backs up checkpoints every 10 minutes\"\"\"\n",
    "    while training_active:\n",
    "        time.sleep(600)  # Wait 10 minutes\n",
    "        \n",
    "        if not training_active:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            results_dir = Path(CONFIG['project']) / CONFIG['name']\n",
    "            last_model = results_dir / 'weights' / 'last.pt'\n",
    "            \n",
    "            if last_model.exists() and CONFIG['drive_backup']:\n",
    "                backup_dir = CONFIG['drive_backup_path']\n",
    "                backup_path = os.path.join(backup_dir, f\"{CONFIG['name']}_last.pt\")\n",
    "                \n",
    "                shutil.copy2(str(last_model), backup_path)\n",
    "                print(f\"\\nüíæ [Auto-Backup] Checkpoint saved to Google Drive at {time.strftime('%H:%M:%S')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  [Auto-Backup] Warning: {e}\")\n",
    "\n",
    "# Initialize backup thread\n",
    "training_active = True\n",
    "\n",
    "if CONFIG['drive_backup']:\n",
    "    backup_thread = threading.Thread(target=periodic_backup, daemon=True)\n",
    "    backup_thread.start()\n",
    "    print(\"üîÑ Background backup thread started (saves to Drive every 10 minutes)\")\n",
    "    print(\"   This provides extra protection against sudden disconnections\\n\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Background backup disabled (drive_backup=False)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Start Training üèãÔ∏è\n",
    "\n",
    "This will take **2-3 hours** on Colab's free T4 GPU.\n",
    "\n",
    "‚ö†Ô∏è **Important:** Keep this tab open or Colab may disconnect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AUTO-RESUME LOGIC (for Colab free tier protection)\n",
    "# ============================================================\n",
    "\n",
    "import shutil\n",
    "\n",
    "# Check for existing checkpoint in Google Drive backup\n",
    "resume_checkpoint = None\n",
    "if CONFIG['auto_resume'] and CONFIG['drive_backup']:\n",
    "    backup_dir = CONFIG['drive_backup_path']\n",
    "    checkpoint_path = os.path.join(backup_dir, f\"{CONFIG['name']}_last.pt\")\n",
    "    \n",
    "    if os.path.exists(checkpoint_path):\n",
    "        resume_checkpoint = checkpoint_path\n",
    "        print(\"üîç CHECKPOINT DETECTED!\")\n",
    "        print(f\"   Found: {checkpoint_path}\")\n",
    "        print(f\"   Training will RESUME from this checkpoint\\n\")\n",
    "    else:\n",
    "        print(\"üîç No previous checkpoint found\")\n",
    "        print(f\"   Will start fresh training\\n\")\n",
    "\n",
    "# Determine which model to use (priority: resume > custom > default)\n",
    "if resume_checkpoint:\n",
    "    # HIGHEST PRIORITY: Resume from interrupted training\n",
    "    model_path = resume_checkpoint\n",
    "    print(f\"‚ôªÔ∏è  RESUMING TRAINING from checkpoint...\")\n",
    "    print(f\"   Checkpoint: {model_path}\")\n",
    "    \n",
    "elif CONFIG['custom_model'] is not None:\n",
    "    # Use custom model for incremental training\n",
    "    model_path = CONFIG['custom_model']\n",
    "    \n",
    "    # Verify custom model exists\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Custom model not found at: {model_path}\\n\"\n",
    "            f\"Please check the path and ensure the model file exists in Google Drive.\"\n",
    "        )\n",
    "    \n",
    "    print(f\"üîÑ INCREMENTAL TRAINING: Loading custom model...\")\n",
    "    print(f\"   Model: {model_path}\")\n",
    "    \n",
    "else:\n",
    "    # Use default base model\n",
    "    model_path = CONFIG['model']\n",
    "    print(f\"üÜï FRESH TRAINING: Loading base model...\")\n",
    "    print(f\"   Model: {model_path}\")\n",
    "\n",
    "model = YOLO(model_path)\n",
    "print(\"‚úÖ Model loaded successfully!\\n\")\n",
    "\n",
    "# Verify dataset path\n",
    "data_yaml = os.path.join(DATASET_PATH, 'data.yaml')\n",
    "if not os.path.exists(data_yaml):\n",
    "    raise FileNotFoundError(f\"‚ùå data.yaml not found at {data_yaml}\")\n",
    "\n",
    "print(f\"üéØ Dataset: {DATASET_PATH}\")\n",
    "print(f\"üìä Config: {data_yaml}\")\n",
    "print(f\"\\nüèãÔ∏è  Starting training for {CONFIG['epochs']} epochs...\\n\")\n",
    "\n",
    "# Create backup directory in Google Drive if it doesn't exist\n",
    "if CONFIG['drive_backup']:\n",
    "    os.makedirs(CONFIG['drive_backup_path'], exist_ok=True)\n",
    "    print(f\"üíæ Checkpoint backup directory ready: {CONFIG['drive_backup_path']}\\n\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=data_yaml,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    imgsz=CONFIG['imgsz'],\n",
    "    batch=CONFIG['batch'],\n",
    "    device=CONFIG['device'],\n",
    "    project=CONFIG['project'],\n",
    "    name=CONFIG['name'],\n",
    "    \n",
    "    # Performance settings\n",
    "    patience=CONFIG['patience'],\n",
    "    save=True,\n",
    "    save_period=CONFIG['save_period'],\n",
    "    \n",
    "    # Optimization\n",
    "    optimizer='auto',\n",
    "    lr0=0.01,\n",
    "    lrf=0.01,\n",
    "    \n",
    "    # Augmentation\n",
    "    hsv_h=0.015,\n",
    "    hsv_s=0.7,\n",
    "    hsv_v=0.4,\n",
    "    degrees=0.0,\n",
    "    translate=0.1,\n",
    "    scale=0.5,\n",
    "    shear=0.0,\n",
    "    perspective=0.0,\n",
    "    flipud=0.0,\n",
    "    fliplr=0.5,\n",
    "    mosaic=1.0,\n",
    "    \n",
    "    # Other\n",
    "    workers=8,\n",
    "    exist_ok=True,\n",
    "    pretrained=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training Complete!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================\n",
    "# SAVE CHECKPOINT TO GOOGLE DRIVE (for auto-resume)\n",
    "# ============================================================\n",
    "if CONFIG['drive_backup']:\n",
    "    print(\"\\nüíæ Backing up checkpoint to Google Drive...\")\n",
    "    \n",
    "    results_dir = Path(CONFIG['project']) / CONFIG['name']\n",
    "    last_model = results_dir / 'weights' / 'last.pt'\n",
    "    best_model = results_dir / 'weights' / 'best.pt'\n",
    "    \n",
    "    backup_dir = CONFIG['drive_backup_path']\n",
    "    \n",
    "    if last_model.exists():\n",
    "        backup_last = os.path.join(backup_dir, f\"{CONFIG['name']}_last.pt\")\n",
    "        shutil.copy2(str(last_model), backup_last)\n",
    "        print(f\"   ‚úÖ Saved: {backup_last}\")\n",
    "    \n",
    "    if best_model.exists():\n",
    "        backup_best = os.path.join(backup_dir, f\"{CONFIG['name']}_best.pt\")\n",
    "        shutil.copy2(str(best_model), backup_best)\n",
    "        print(f\"   ‚úÖ Saved: {backup_best}\")\n",
    "    \n",
    "    print(f\"\\nüõ°Ô∏è  Your progress is safe in Google Drive!\")\n",
    "    print(f\"   If this session disconnects, re-run the notebook to resume.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Validate the Model\n",
    "\n",
    "Run validation on the test set to check performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the best model\n",
    "results_dir = Path(CONFIG['project']) / CONFIG['name']\n",
    "best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "\n",
    "print(f\"üîç Validating model: {best_model_path}\\n\")\n",
    "\n",
    "# Load best model\n",
    "best_model = YOLO(str(best_model_path))\n",
    "\n",
    "# Validate\n",
    "metrics = best_model.val()\n",
    "\n",
    "# Display metrics\n",
    "print(\"\\nüìà Validation Metrics:\")\n",
    "print(f\"   mAP50: {metrics.box.map50:.3f}\")\n",
    "print(f\"   mAP50-95: {metrics.box.map:.3f}\")\n",
    "print(f\"   Precision: {metrics.box.mp:.3f}\")\n",
    "print(f\"   Recall: {metrics.box.mr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test on Sample Images\n",
    "\n",
    "Test the trained model on a few validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Get sample images from validation set\n",
    "val_images = glob.glob(os.path.join(DATASET_PATH, 'valid/images/*'))[:5]\n",
    "\n",
    "print(f\"üß™ Testing on {len(val_images)} sample images...\\n\")\n",
    "\n",
    "for img_path in val_images:\n",
    "    print(f\"üì∏ Testing: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    # Run inference\n",
    "    results = best_model(img_path)\n",
    "    \n",
    "    # Display results\n",
    "    for result in results:\n",
    "        boxes = result.boxes\n",
    "        print(f\"   Detected {len(boxes)} objects:\")\n",
    "        for box in boxes:\n",
    "            cls = int(box.cls[0])\n",
    "            conf = float(box.conf[0])\n",
    "            name = best_model.names[cls]\n",
    "            print(f\"      - {name}: {conf:.2%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: View Training Results\n",
    "\n",
    "Display training curves and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "results_dir = Path(CONFIG['project']) / CONFIG['name']\n",
    "\n",
    "print(\"üìä Training Results:\\n\")\n",
    "\n",
    "# Display results plot\n",
    "results_plot = results_dir / 'results.png'\n",
    "if results_plot.exists():\n",
    "    print(\"üìà Training Curves:\")\n",
    "    display(Image(filename=str(results_plot)))\n",
    "\n",
    "# Display confusion matrix\n",
    "confusion_matrix = results_dir / 'confusion_matrix.png'\n",
    "if confusion_matrix.exists():\n",
    "    print(\"\\nüéØ Confusion Matrix:\")\n",
    "    display(Image(filename=str(confusion_matrix)))\n",
    "\n",
    "# Display labels\n",
    "labels_plot = results_dir / 'labels.jpg'\n",
    "if labels_plot.exists():\n",
    "    print(\"\\nüè∑Ô∏è  Dataset Labels Distribution:\")\n",
    "    display(Image(filename=str(labels_plot)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Download Trained Model\n",
    "\n",
    "Download the trained model to use in your local detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "results_dir = Path(CONFIG['project']) / CONFIG['name']\n",
    "best_model_path = results_dir / 'weights' / 'best.pt'\n",
    "last_model_path = results_dir / 'weights' / 'last.pt'\n",
    "\n",
    "print(\"üì• Preparing models for download...\\n\")\n",
    "\n",
    "# Create a zip file with all results\n",
    "output_zip = f\"{CONFIG['name']}_trained_model.zip\"\n",
    "shutil.make_archive(output_zip.replace('.zip', ''), 'zip', str(results_dir))\n",
    "\n",
    "print(f\"‚úÖ Created: {output_zip}\")\n",
    "print(f\"üì¶ Size: {os.path.getsize(output_zip) / (1024*1024):.2f} MB\\n\")\n",
    "\n",
    "# Download the zip\n",
    "print(\"‚¨áÔ∏è  Downloading...\")\n",
    "files.download(output_zip)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Download Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìù Next Steps:\")\n",
    "print(\"1. Extract the zip file on your local machine\")\n",
    "print(\"2. Copy 'weights/best.pt' to your project folder\")\n",
    "print(\"3. Update electronics_detector.py:\")\n",
    "print(\"   self.model = YOLO('path/to/best.pt')\")\n",
    "print(\"4. Run your detector!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: (Optional) Save to Google Drive\n",
    "\n",
    "Save the trained model and results to Google Drive for long-term storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run if you want to save to Google Drive\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# results_dir = Path(CONFIG['project']) / CONFIG['name']\n",
    "# drive_save_path = '/content/drive/MyDrive/electronics_training_results'\n",
    "\n",
    "# print(f\"üíæ Copying results to Google Drive...\")\n",
    "# shutil.copytree(str(results_dir), drive_save_path, dirs_exist_ok=True)\n",
    "\n",
    "# print(f\"‚úÖ Results saved to: {drive_save_path}\")\n",
    "# print(\"\\nYour results are now permanently saved in Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ Training Complete!\n",
    "\n",
    "### Summary:\n",
    "- ‚úÖ Model trained on GPU (much faster than CPU)\n",
    "- ‚úÖ Best weights saved: `weights/best.pt`\n",
    "- ‚úÖ Results downloaded or saved to Google Drive\n",
    "\n",
    "### Using Your Trained Model:\n",
    "\n",
    "```python\n",
    "# In electronics_detector.py, replace:\n",
    "self.model = YOLO('yolov8n.pt')\n",
    "\n",
    "# With:\n",
    "self.model = YOLO('path/to/your/best.pt')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources:\n",
    "- [YOLOv8 Documentation](https://docs.ultralytics.com/)\n",
    "- [Roboflow Universe Datasets](https://universe.roboflow.com/)\n",
    "- [Project GitHub](https://github.com/GAM3RG33K/electronics_detector)\n",
    "\n",
    "---\n",
    "\n",
    "**Need Help?** Check the README.md or open an issue on GitHub."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
